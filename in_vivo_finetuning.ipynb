{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f5b3c178",
   "metadata": {},
   "source": [
    "## Library imoprt & Function definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a3bfc5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import copy\n",
    "import dgl\n",
    "import torch\n",
    "from dgllife.utils import CanonicalAtomFeaturizer, CanonicalBondFeaturizer\n",
    "from dgllife.utils import smiles_to_bigraph\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, WeightedRandomSampler\n",
    "from tqdm import tqdm\n",
    "from torch.autograd import Variable\n",
    "from model.main.DMPNN import * \n",
    "from model.main.utils import *\n",
    "from model.main.scheduler import NoamLR\n",
    "from model.main.models import *\n",
    "from model.main.min_norm_solvers import MinNormSolver, gradient_normalizers\n",
    "from model.main.trainer import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "280bf2a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def warn(*args, **kwargs):\n",
    "    pass\n",
    "import warnings\n",
    "warnings.warn = warn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d5f3928",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mtl_building(node_input_dim=74,\n",
    "                      edge_input_dim=12,\n",
    "                      node_hidden_dim=int(2 ** 7),\n",
    "                      edge_hidden_dim=int(2 ** 7),\n",
    "                      num_step_message_passing=4,\n",
    "                      num_step_mha=1, tox21_task_num = 12, task_num = 3):\n",
    "    \n",
    "    model_chembl = ChEMBL_fullmodel(\n",
    "                 node_input_dim=node_input_dim,\n",
    "                 edge_input_dim=edge_input_dim,\n",
    "                 node_hidden_dim=node_hidden_dim,\n",
    "                 edge_hidden_dim=edge_hidden_dim,\n",
    "                 num_step_message_passing=num_step_message_passing, \n",
    "                 num_step_mha=num_step_mha)    \n",
    "    \n",
    "    model_tox21 = Tox21_embed(\n",
    "                 node_input_dim=node_input_dim,\n",
    "                 edge_input_dim=edge_input_dim,\n",
    "                 node_hidden_dim=node_hidden_dim,\n",
    "                 edge_hidden_dim=edge_hidden_dim,\n",
    "                 num_step_message_passing=num_step_message_passing, \n",
    "                 num_step_mha=num_step_mha,\n",
    "                 task_num = tox21_task_num)\n",
    "    \n",
    "    model = MTL_invivo(\n",
    "                 node_input_dim=node_input_dim,\n",
    "                 edge_input_dim=edge_input_dim,\n",
    "                 node_hidden_dim=node_hidden_dim,\n",
    "                 edge_hidden_dim=edge_hidden_dim,\n",
    "                 num_step_message_passing=num_step_message_passing, \n",
    "                 num_step_mha=num_step_mha,\n",
    "                tox21_task_num = tox21_task_num,\n",
    "                task_num = task_num)\n",
    "    \n",
    "    return model_chembl, model_tox21, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63868d58",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tox21_collate(sample):\n",
    "    graphs, mask = map(list,zip(*sample))\n",
    "    batched_graph = dgl.batch(graphs)\n",
    "    batched_graph.set_n_initializer(dgl.init.zero_initializer)\n",
    "    batched_graph.set_e_initializer(dgl.init.zero_initializer)\n",
    "    return batched_graph, mask\n",
    "\n",
    "def tox21_load_data(df, labels, atom_featurizer, bond_featurizer):\n",
    "    print(\"---------------- Target loading --------------------\")\n",
    "    test_g = [smiles_to_bigraph(smi, node_featurizer=atom_featurizer, edge_featurizer=bond_featurizer) for smi in df['smiles']]\n",
    "    mask = np.array(df[labels].notna()).astype('int').tolist()\n",
    "    test_data = list(zip(test_g, mask))\n",
    "    print(\"---------------- Target loading complete --------------------\")\n",
    "    return test_data\n",
    "\n",
    "def dataloader_tox21(train, labels, batch_size):\n",
    "    s_tr = tox21_load_data(train, labels, CanonicalAtomFeaturizer(), CanonicalBondFeaturizer())\n",
    "    tr_loader = DataLoader(s_tr, batch_size=batch_size, shuffle = False, collate_fn = tox21_collate, drop_last = False)\n",
    "\n",
    "    return tr_loader\n",
    "\n",
    "def collate(sample):\n",
    "    graphs, labels, embeds, mask = map(list,zip(*sample))\n",
    "    batched_graph = dgl.batch(graphs)\n",
    "    batched_graph.set_n_initializer(dgl.init.zero_initializer)\n",
    "    batched_graph.set_e_initializer(dgl.init.zero_initializer)\n",
    "    return batched_graph, torch.tensor(labels), embeds, torch.tensor(mask)\n",
    "\n",
    "def load_data(df, labels, embed, atom_featurizer, bond_featurizer):\n",
    "    print(\"---------------- Target loading --------------------\")\n",
    "    test_g = [smiles_to_bigraph(smi, node_featurizer=atom_featurizer, edge_featurizer=bond_featurizer) for smi in df['smiles']]\n",
    "    test_y = df[labels].values.tolist()\n",
    "    mask = np.array(df[labels].notna()).astype('int').tolist()\n",
    "    embed_list = [embed[i, :, :] for i in range(len(df))]\n",
    "    test_data = list(zip(test_g, test_y, embed_list, mask))\n",
    "    print(\"---------------- Target loading complete --------------------\")\n",
    "    return test_data\n",
    "\n",
    "def dataloader_train(train, valid, labels, tr_embeds, va_embeds, batch_size, sampler):\n",
    "    s_tr = load_data(train, labels, tr_embeds, CanonicalAtomFeaturizer(), CanonicalBondFeaturizer())\n",
    "    s_va =load_data(valid, labels, va_embeds, CanonicalAtomFeaturizer(), CanonicalBondFeaturizer())\n",
    "    if sampler is not None:\n",
    "        tr_loader = DataLoader(s_tr, batch_size=batch_size, shuffle = False, collate_fn = collate, drop_last = True, sampler = sampler)\n",
    "    else:\n",
    "        tr_loader = DataLoader(s_tr, batch_size=batch_size, shuffle = True, collate_fn = collate, drop_last = False)\n",
    "    vr_loader = DataLoader(s_va, batch_size=batch_size, shuffle = False, collate_fn = collate, drop_last = False)\n",
    "\n",
    "    return tr_loader, vr_loader\n",
    "\n",
    "def dataloader_test(train, labels, embeds, batch_size):\n",
    "    s_tr = load_data(train, labels, embeds, CanonicalAtomFeaturizer(), CanonicalBondFeaturizer())\n",
    "    tr_loader = DataLoader(s_tr, batch_size=batch_size, shuffle = False, collate_fn = collate, drop_last = False)\n",
    "\n",
    "    return tr_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42a52a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tox21_embed_calculate(model_pred, tr_loader, device):\n",
    "\n",
    "    graph_out_list = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        model_pred.eval()\n",
    "\n",
    "        for num, (tox_g, tox_mask) in enumerate(tr_loader):\n",
    "\n",
    "            tox_g = tox_g.to(device)\n",
    "            tox_atom = tox_g.ndata.pop('h').to(device)\n",
    "            tox_bond = tox_g.edata.pop('e').to(device)\n",
    "            outputs = model_pred.forward(tox_g, tox_atom, tox_bond)\n",
    "\n",
    "            graph_out_list.append(outputs.detach().to('cpu').numpy())\n",
    "            \n",
    "    total_graph_out = torch.tensor(np.vstack(graph_out_list))\n",
    "    \n",
    "    return total_graph_out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f13fc3e4",
   "metadata": {},
   "source": [
    "## Model configuration & Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "217f8fa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "GPU_NUM = 0\n",
    "device = torch.device(f'cuda:{GPU_NUM}' if GPU_NUM >= 0 else 'cpu')\n",
    "torch.cuda.set_device(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2e4cc3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = './data/internal_data/In_vivo/'\n",
    "df_train = pd.read_csv(data_path + 'invivo_train.csv')\n",
    "df_valid = pd.read_csv(data_path + 'invivo_valid.csv')\n",
    "df_test = pd.read_csv(data_path + 'invivo_test.csv')\n",
    "assay_list = list(df_train.columns[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30c642ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed: int = 42) :\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    print(f\"Random seed set as {seed}\")\n",
    "    \n",
    "\n",
    "seed = 109\n",
    "set_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9921dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "n_epochs = 60\n",
    "lr = 1e-4\n",
    "warmup_epoch = 3\n",
    "decay_step = 13\n",
    "weight_decay = 1e-6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf04f48d",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_task = len(assay_list)\n",
    "mode = 'invivo'\n",
    "loss_list = []\n",
    "for i, col in enumerate(assay_list):\n",
    "    loss_weighted = weight_crossentropy(df_train, col, mode, device)\n",
    "    loss_list.append(loss_weighted)\n",
    "\n",
    "src_model, tox21_model, model = mtl_building()\n",
    "\n",
    "state = torch.load(\"./model/pretrained_ckpts/chembl.pth\", map_location=device)\n",
    "src_model = src_model.to(device)\n",
    "src_model.load_state_dict(state['model_state_dict'], strict = False)\n",
    "\n",
    "state = torch.load(\"./model/pretrained_ckpts/tox21.pth\", map_location=device)\n",
    "tox21_model = tox21_model.to(device)\n",
    "tox21_model.load_state_dict(state['model_state_dict'], strict = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6cefa61",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to(device)\n",
    "model.invivo_gnn.load_state_dict(copy.deepcopy(src_model.featurizer.state_dict()))\n",
    "for i in range(3):\n",
    "    model.invivo_readout[i].load_state_dict(copy.deepcopy(src_model.readout.state_dict()))\n",
    "\n",
    "sample_weights = calculate_sample_weights(np.array(df_train[assay_list].notna()).astype('int'))\n",
    "sampler = WeightedRandomSampler(sample_weights.type('torch.DoubleTensor'), num_samples = len(sample_weights))\n",
    "\n",
    "tr_tox21_loader = dataloader_tox21(df_train, assay_list, 100)\n",
    "va_tox21_loader = dataloader_tox21(df_valid, assay_list, 100)\n",
    "ts_tox21_loader = dataloader_tox21(df_test, assay_list, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c74438e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_tox21_embed = tox21_embed_calculate(tox21_model, tr_tox21_loader, device)\n",
    "va_tox21_embed = tox21_embed_calculate(tox21_model, va_tox21_loader, device)\n",
    "ts_tox21_embed = tox21_embed_calculate(tox21_model, ts_tox21_loader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "790c644a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_loader, va_loader = dataloader_train(df_train, df_valid, assay_list, tr_tox21_embed, va_tox21_embed, batch_size, sampler = sampler)\n",
    "data_NT = len(tr_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d023fda",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = f'./invivo_ckpts/'\n",
    "\n",
    "model_optimizer = torch.optim.Adam(model.parameters(), lr = lr, weight_decay = weight_decay)\n",
    "model_scheduler = NoamLR(optimizer = model_optimizer,\n",
    "         warmup_epochs = [warmup_epoch],\n",
    "         total_epochs = [decay_step],\n",
    "         steps_per_epoch = data_NT,\n",
    "         init_lr = [1e-5],\n",
    "         max_lr = [lr],\n",
    "         final_lr = [3e-5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b08badb",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_dict, top_epoch = invivo_model_train(model, model_path, tr_loader, va_loader, model_optimizer, model_scheduler, loss_list, device, \n",
    "              seed, epochs = n_epochs)\n",
    "print(\"Finished at :{}\".format(top_epoch))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82dcdae5",
   "metadata": {},
   "source": [
    "## Performance evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c365891a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ts_loader = dataloader_test(df_test, assay_list, ts_tox21_embed, 100)\n",
    "\n",
    "state = torch.load(\"./invivo_ckpts/seed109/epoch_36.pth\", map_location=device)\n",
    "model.load_state_dict(state['model_state_dict'])\n",
    "test_pred = invivo_model_test(model, ts_loader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebb65ad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_results(test_pred, df_valid, assay_list, score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2afe8747",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mttox_test",
   "language": "python",
   "name": "mttox_test"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
